{
  "metadata": {
    "saveOutput": false,
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 構造化ストリーミングを使用した Azure Cosmos DB コレクションへのストリーミング取り込み\n",
        "\n",
        "このノートでは、あなたは、\n",
        "\n",
        "1. レートストリーミングソースを使用したストリーミングデータ生成のシミュレーションします\n",
        "2. IoTSignals スキーマに従ってストリームデータフレームをフォーマットする\n",
        "3. ストリーミングデータフレームを Azure Cosmos DB コレクションに書き込みます\n",
        "\n",
        ">**ご存知ですか？** Azure Cosmos DB は、IoT の予測メンテナンスと異常検出のユースケースに最適です。Azure Cosmos DB 用の Azure Synapse Link の HTAP 機能を活用した IoT アーキテクチャの詳細については、[こちらをクリック](https://docs.microsoft.com/ja-jp/azure/cosmos-db/synapse-link-use-cases)してください。\n",
        "\n",
        ">**ご存知ですか？** [Azure Cosmos DB 用 Azure Synapse Link](https://docs.microsoft.com/ja-jp/azure/cosmos-db/synapse-link) は、Azure Cosmos DBの運用データに対してほぼリアルタイムの分析を実行できるハイブリッドトランザクション分析処理 (HTAP) 機能です。\n",
        "&nbsp;\n",
        "\n",
        ">**ご存知ですか？** [Azure Cosmos DB 分析ストア](https://docs.microsoft.com/ja-jp/azure/cosmos-db/analytical-store-introduction)は、トランザクションワークロードに影響を与えることなく、Azure Cosmos DB の運用データに対して大規模な分析を可能にする完全に分離された列ストアです。\n",
        "&nbsp;"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. レートストリーミングソースを使用したストリーミングデータ生成をシミュレーションする\n",
        "* レートストリーミングソースは、ここでのソリューションを簡略化するために使用され、[Azure Event Hubs](https://azure.microsoft.com/ja-jp/services/event-hubs/) や [Apache Kafka](https://docs.microsoft.com/ja-jp/azure/hdinsight/kafka/apache-kafka-introduction) などのサポートされているストリーミングソースに置き換えることができます。\n",
        "\n",
        "* [こちらをクリック](https://github.com/Azure-Samples/streaming-at-scale)し、さまざまな Azure テクノロジーを選択して実装可能なエンドツーエンドのストリーミングソリューションの方法の詳細をご覧ください。\n",
        "\n",
        ">**ご存知ですか？**  レートストリーミングソースは、指定された 1 秒あたりの行数でデータを生成し、各出力行にはタイムスタンプと値が含まれます。"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "dfStream = (spark\n",
        "                .readStream\n",
        "                .format(\"rate\")\n",
        "                .option(\"rowsPerSecond\", 10)\n",
        "                .load()\n",
        "            )"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. IoTSignals スキーマに従いストリームデータフレームをフォーマットする\n"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import StringType\n",
        "import uuid\n",
        "\n",
        "numberOfDevices = 10\n",
        "generate_uuid = F.udf(lambda : str(uuid.uuid4()), StringType())\n",
        "              \n",
        "dfIoTSignals = (dfStream\n",
        "                    .withColumn(\"id\", generate_uuid())\n",
        "                    .withColumn(\"deviceId\", F.concat(F.lit(\"dev-\"), F.expr(\"mod(value, %d)\" % numberOfDevices)))\n",
        "                    .withColumn(\"dateTime\", dfStream[\"timestamp\"].cast(StringType()))\n",
        "                    .withColumn(\"unit\", F.expr(\"CASE WHEN rand() < 0.5 THEN 'Revolutions per Minute' ELSE 'MegaWatts' END\"))\n",
        "                    .withColumn(\"unitSymbol\", F.expr(\"CASE WHEN rand() < 0.5 THEN 'RPM' ELSE 'MW' END\"))\n",
        "                    .withColumn(\"measureType\", F.expr(\"CASE WHEN rand() < 0.5 THEN 'Rotation Speed' ELSE 'Output' END\"))\n",
        "                    .withColumn(\"measureValue\", F.expr(\"CASE WHEN rand() > 0.95 THEN value * 10 WHEN rand() < 0.05 THEN value div 10 ELSE value END\"))\n",
        "                    .drop(\"timestamp\", \"value\")\n",
        "                )"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Azure Cosmos DB コレクションへのストリーム書き込みを行う\n",
        ">**ご存知ですか？** 「cosmos.oltp」は、Cosmos DB トランザクションストアへの接続を可能にする Spark フォーマットです。\n",
        "\n",
        ">**ご存知ですか？** Cosmos DB コレクションへの取り込みは、分析ストアが有効になっているかどうかに関係なく、常にトランザクションストアを通じて実行されます。"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "streamQuery = dfIoTSignals\\\n",
        "                    .writeStream\\\n",
        "                    .format(\"cosmos.oltp\")\\\n",
        "                    .outputMode(\"append\")\\\n",
        "                    .option(\"spark.cosmos.connection.mode\", \"gateway\") \\\n",
        "                    .option(\"spark.synapse.linkedService\", \"CosmosDBIoTDemo\")\\\n",
        "                    .option(\"spark.cosmos.container\", \"IoTSignals\")\\\n",
        "                    .option(\"checkpointLocation\", \"/writeCheckpointDir\")\\\n",
        "                    .start()\n",
        "\n",
        "streamQuery.awaitTermination()"
      ],
      "attachments": {}
    }
  ]
}