{
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "sessionKeepAliveTimeout": 30,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Getting started with Azure Cosmos DB's API for MongoDB and Synapse Link\n",
        "\n",
        "In this sample we will execute the following tasks:\n",
        "\n",
        "1. Insert a dataset using the traditional MongoDB client.\n",
        "1. Execute aggregation queries against the Analytical Store from the transactional data we inserted.\n",
        "1. Insert another dataset, but this time using the MongoSpark connector.\n",
        "1. Execute aggregation queries again, consolidating both datasets.\n",
        "\n",
        "## Pre-requisites\n",
        "1. Have you created a MongoDB API account in Azure Cosmos DB? If not, go to [Create an account for Azure Cosmos DB's API for MongoDB]().\n",
        "1. For your Cosmos DB account, have you enabled Synapse Link? If not, go to [Enable Synapse Link for Azure Cosmos DB's API for MongoDB]().\n",
        "1. Have you created a Synapse Workspace? If not, go to [Create Synapse Workspace account](). Please don't forget to add yourself as **Storage Blob Data Contributor** to the primary ADLS G2 account that is linked to the Synapse workspace.\n",
        "\n",
        "## Create a Cosmos DB collection with Synapse Link\n",
        "\n",
        "Please be careful, all commands are case sensitive.\n",
        "\n",
        "1. Create a database named `DemoSynapseLinkMongoDB`. \n",
        "1. Create a collection named `HTAP` with a partition key called `item`. Make sure you set the `Analytical store` option to `On` when you create your collection.\n",
        "\n",
        "## Optional - Connect your collection to Synapse\n",
        "\n",
        "To accelerate future work, you can connect your collection to Synapse. **We won't use this capability in this demo**, but fell free to test and use it.\n",
        "\n",
        "1. Go to your Synapse Analytics workspace.\n",
        "1. Create a `Linked Data` connection for your MongoDB API account. \n",
        "    1. Under the `Data` blade, select the + (plus) sign.\n",
        "    1. Select the `Connect to external data` option.\n",
        "    1. Now select the `Azure Cosmos DB (MongoDB API)` option. \n",
        "    1. Enter all the information regarding your specific Azure Cosmos DB account either by using the dropdowns or by entering the connection string. Take note of the name you assigned to your `Linked Data` connection. \n",
        "    - Alternatively, you can also use the connection parameters from your account overview.\n",
        "1. Test the connection by looking for your database accounts in the `Data` blade, and under the `Linked` tab.\n",
        "    - There should be a list that contains all accounts and collections.\n",
        "    - Collections that have an `Analytical Store` enabled will have a distinctive icon."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Let's get the environment ready\n",
        "\n",
        "This environment allows you to install and use any python libraries that you want to run. For this sample, you need to add the following libraries to your Spark pool:\n",
        "\n",
        "```\n",
        "pymongo==2.8.1\n",
        "aenum==2.2.4\n",
        "bson==0.5.10\n",
        "```\n",
        "\n",
        "Learn how to import libraries into your Spark pools in [this article](https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-azure-portal-add-libraries). Please use the `requirements.txt` file located in the same folder of this notebook to update your pool packages.\n",
        "\n",
        "You can execute the following command to make sure all the libraries are installed correctly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "rosouzMongo",
              "session_id": 25,
              "statement_id": 67,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2020-10-21T19:39:01.0593686Z",
              "execution_start_time": "2020-10-21T19:39:01.0850441Z",
              "execution_finish_time": "2020-10-21T19:39:03.1179478Z"
            },
            "text/plain": "StatementMeta(rosouzMongo, 25, 67, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "execution_count": 65,
          "output_type": "execute_result",
          "data": {
            "text/plain": "pymongo OK\nbson OK\naenum OK\nbackports-abc NOK"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "import importlib\r\n",
        "\r\n",
        "packages = ['pymongo','bson','aenum']\r\n",
        "for package in packages:\r\n",
        "    test = importlib.util.find_spec(package)\r\n",
        "    if test:\r\n",
        "        print(package, \"OK\")\r\n",
        "    else:\r\n",
        "        print(package, \"NOK\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Add your database account and collection details here!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "rosouzMongo",
              "session_id": 25,
              "statement_id": 68,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2020-10-21T19:39:01.1553018Z",
              "execution_start_time": "2020-10-21T19:39:03.1414748Z",
              "execution_finish_time": "2020-10-21T19:39:05.1760382Z"
            },
            "text/plain": "StatementMeta(rosouzMongo, 25, 68, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "execution_count": 66,
          "output_type": "execute_result",
          "data": {
            "text/plain": ""
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "DATABASE_ACCOUNT_NAME = '<your-azure-cosmos-db-mongodb-api-account-name'\n",
        "DATABASE_ACCOUNT_READWRITE_KEY = 'your-azure-cosmos-db-mongodb-api-account-key'\n",
        "DATABASE_NAME = 'DemoSynapseLinkMongoDB'\n",
        "COLLECTION_NAME = 'HTAP'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Let's initialize the MongoDB client\n",
        "\n",
        "You are only going to need the following parameters from your account overview: \n",
        "- Connection string.\n",
        "- Primary or secondary ready/write key.\n",
        "\n",
        "Remember that we named our database `DemoSynapseLinkMongoDB` and our collection `HTAP`.\n",
        "\n",
        "The code snippet below shows how to initialize the `MongoClient` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "rosouzMongo",
              "session_id": 25,
              "statement_id": 69,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2020-10-21T19:39:01.2329772Z",
              "execution_start_time": "2020-10-21T19:39:05.2011614Z",
              "execution_finish_time": "2020-10-21T19:39:07.2360305Z"
            },
            "text/plain": "StatementMeta(rosouzMongo, 25, 69, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "execution_count": 67,
          "output_type": "execute_result",
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "from pymongo import MongoClient\n",
        "from bson import ObjectId # For ObjectId to work\n",
        "\n",
        "client = MongoClient(\"mongodb://{account}.mongo.cosmos.azure.com:10255/?ssl=true&replicaSet=globaldb\".format(account = DATABASE_ACCOUNT_NAME)) # Your own database account endpoint.\n",
        "db = client.DemoSynapseLinkMongoDB    # Select the database\n",
        "db.authenticate(name=DATABASE_ACCOUNT_NAME,password=DATABASE_ACCOUNT_READWRITE_KEY) # Use your database account name and any of your read/write keys."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inserting data with the MongoClient driver\n",
        "\n",
        "The following sample will generate 500 items based on random data. Each item will contain the following fields:\n",
        "- item, string\n",
        "- price, float\n",
        "- rating, integer\n",
        "- timestamp, [epoch integer](http://unixtimestamp.50x.eu/about.php)\n",
        "\n",
        "This data will be inserted into the MongoDB store of your database. This emulates the transactional data that an application would generate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "rosouzMongo",
              "session_id": 25,
              "statement_id": 70,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2020-10-21T19:39:01.3286937Z",
              "execution_start_time": "2020-10-21T19:39:07.2640005Z",
              "execution_finish_time": "2020-10-21T19:39:23.4933385Z"
            },
            "text/plain": "StatementMeta(rosouzMongo, 25, 70, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "execution_count": 68,
          "output_type": "execute_result",
          "data": {
            "text/plain": "finished creating 500 orders"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "from random import randint\n",
        "import time\n",
        "\n",
        "orders = db[\"HTAP\"]\n",
        "\n",
        "items = ['Pizza','Sandwich','Soup', 'Salad', 'Tacos']\n",
        "prices = [2.99, 3.49, 5.49, 12.99, 54.49]\n",
        "\n",
        "for x in range(1, 501):\n",
        "    order = {\n",
        "        'item' : items[randint(0, (len(items)-1))],\n",
        "        'price' : prices[randint(0, (len(prices)-1))],\n",
        "        'rating' : randint(1, 5),\n",
        "        'timestamp' : time.time()\n",
        "    }\n",
        "    \n",
        "    result=orders.insert(order)\n",
        "\n",
        "print('finished creating 500 orders')\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Read data from the Analytical Store.\n",
        "\n",
        "Now that we have inserted some transactional data, let's read it from Azure Cosmos DB analytical store. Cosmos DB will automatically transform the BSON data (Binary JSON) into a columnar format, which will make it fast and easy to execute aggregation workloads on top of your transactional data, at no RUs or performance costs.\n",
        "\n",
        "The cells below will:\n",
        "\n",
        "1. Load the data from analytical store into a DataFrame.\n",
        "1. Check the top 10 rows. Yes, the BSON data was converted into columar structured format.\n",
        "1. Run aggregations\n",
        "1. Check the DataFrame schema.\n",
        "\n",
        "\n",
        "> If you get an \"no snapshot\" error, please wait a couple of minutes because the root cause is that the auto sync between transactional and analytical stores isn't completed yet. This process usually takes up to 2 minutes, but in some cases it may take up to 5 minutes.\n",
        "\n",
        "**Important: Please note that we are using random values for prices and ratings. Don't expect the same results of the outputs below. What you can expect is the same behavior and experience.**\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "rosouzMongo",
              "session_id": 25,
              "statement_id": 71,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2020-10-21T19:39:01.4103619Z",
              "execution_start_time": "2020-10-21T19:39:23.5203192Z",
              "execution_finish_time": "2020-10-21T19:39:25.5523996Z"
            },
            "text/plain": "StatementMeta(rosouzMongo, 25, 71, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "execution_count": 69,
          "output_type": "execute_result",
          "data": {
            "text/plain": "[Row(_rid='JYAqAKy0s3+eDAAAAAAAAA==', _ts=1603308932, id='NWY5MDhkODQ2NWYwNjMzYjI4Y2VhMGUz', _etag='\"17003d58-0000-0800-0000-5f908d840000\"', _id=Row(objectId='_???e?c;(??'), item=Row(string='Salad'), price=Row(float64=54.49), rating=Row(int32=5), timestamp=Row(float64=1603308932.6953204, string=None), _partitionKey=Row(string='JYAqAKy0s38=')), Row(_rid='JYAqAKy0s3+fDAAAAAAAAA==', _ts=1603308932, id='NWY5MDhkODQ2NWYwNjMzYjI4Y2VhMGU0', _etag='\"17003e58-0000-0800-0000-5f908d840000\"', _id=Row(objectId='_???e?c;(??'), item=Row(string='Salad'), price=Row(float64=12.99), rating=Row(int32=4), timestamp=Row(float64=1603308932.766245, string=None), _partitionKey=Row(string='JYAqAKy0s38=')), Row(_rid='JYAqAKy0s3+gDAAAAAAAAA==', _ts=1603308932, id='NWY5MDhkODQ2NWYwNjMzYjI4Y2VhMGU1', _etag='\"17003f58-0000-0800-0000-5f908d840000\"', _id=Row(objectId='_???e?c;(??'), item=Row(string='Pizza'), price=Row(float64=12.99), rating=Row(int32=2), timestamp=Row(float64=1603308932.7732844, string=None), _partitionKey=Row(string='JYAqAKy0s38=')), Row(_rid='JYAqAKy0s3+hDAAAAAAAAA==', _ts=1603308932, id='NWY5MDhkODQ2NWYwNjMzYjI4Y2VhMGU2', _etag='\"17004058-0000-0800-0000-5f908d840000\"', _id=Row(objectId='_???e?c;(??'), item=Row(string='Pizza'), price=Row(float64=12.99), rating=Row(int32=5), timestamp=Row(float64=1603308932.798045, string=None), _partitionKey=Row(string='JYAqAKy0s38=')), Row(_rid='JYAqAKy0s3+iDAAAAAAAAA==', _ts=1603308932, id='NWY5MDhkODQ2NWYwNjMzYjI4Y2VhMGU3', _etag='\"17004158-0000-0800-0000-5f908d840000\"', _id=Row(objectId='_???e?c;(??'), item=Row(string='Soup'), price=Row(float64=5.49), rating=Row(int32=2), timestamp=Row(float64=1603308932.8280613, string=None), _partitionKey=Row(string='JYAqAKy0s38=')), Row(_rid='JYAqAKy0s3+jDAAAAAAAAA==', _ts=1603308932, id='NWY5MDhkODQ2NWYwNjMzYjI4Y2VhMGU4', _etag='\"17004258-0000-0800-0000-5f908d840000\"', _id=Row(objectId='_???e?c;(??'), item=Row(string='Soup'), price=Row(float64=54.49), rating=Row(int32=4), timestamp=Row(float64=1603308932.8595922, string=None), _partitionKey=Row(string='JYAqAKy0s38=')), Row(_rid='JYAqAKy0s3+kDAAAAAAAAA==', _ts=1603308932, id='NWY5MDhkODQ2NWYwNjMzYjI4Y2VhMGU5', _etag='\"17004358-0000-0800-0000-5f908d840000\"', _id=Row(objectId='_???e?c;(??'), item=Row(string='Sandwich'), price=Row(float64=12.99), rating=Row(int32=4), timestamp=Row(float64=1603308932.8912776, string=None), _partitionKey=Row(string='JYAqAKy0s38=')), Row(_rid='JYAqAKy0s3+lDAAAAAAAAA==', _ts=1603308932, id='NWY5MDhkODQ2NWYwNjMzYjI4Y2VhMGVh', _etag='\"17004458-0000-0800-0000-5f908d840000\"', _id=Row(objectId='_???e?c;(??'), item=Row(string='Soup'), price=Row(float64=54.49), rating=Row(int32=5), timestamp=Row(float64=1603308932.9221158, string=None), _partitionKey=Row(string='JYAqAKy0s38=')), Row(_rid='JYAqAKy0s3+mDAAAAAAAAA==', _ts=1603308932, id='NWY5MDhkODQ2NWYwNjMzYjI4Y2VhMGVi', _etag='\"17004558-0000-0800-0000-5f908d840000\"', _id=Row(objectId='_???e?c;(??'), item=Row(string='Sandwich'), price=Row(float64=54.49), rating=Row(int32=2), timestamp=Row(float64=1603308932.9531987, string=None), _partitionKey=Row(string='JYAqAKy0s38=')), Row(_rid='JYAqAKy0s3+nDAAAAAAAAA==', _ts=1603308933, id='NWY5MDhkODQ2NWYwNjMzYjI4Y2VhMGVj', _etag='\"17004658-0000-0800-0000-5f908d850000\"', _id=Row(objectId='_???e?c;(??'), item=Row(string='Sandwich'), price=Row(float64=5.49), rating=Row(int32=4), timestamp=Row(float64=1603308932.9927793, string=None), _partitionKey=Row(string='JYAqAKy0s38='))]"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "# Load the data from analytical store into a DataFrame.\n",
        "df = spark.read.format(\"cosmos.olap\")\\\n",
        "    .option(\"spark.cosmos.accountEndpoint\", \"https://{account}.documents.azure.com:443/\".format(account = DATABASE_ACCOUNT_NAME))\\\n",
        "    .option(\"spark.cosmos.accountKey\", DATABASE_ACCOUNT_READWRITE_KEY)\\\n",
        "    .option(\"spark.cosmos.database\", DATABASE_NAME)\\\n",
        "    .option(\"spark.cosmos.container\", COLLECTION_NAME)\\\n",
        "    .load()\n",
        "\n",
        "# Check the top 10 rows\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "rosouzMongo",
              "session_id": 25,
              "statement_id": 72,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2020-10-21T19:39:01.4852334Z",
              "execution_start_time": "2020-10-21T19:39:25.5723405Z",
              "execution_finish_time": "2020-10-21T19:39:27.6004358Z"
            },
            "text/plain": "StatementMeta(rosouzMongo, 25, 72, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "execution_count": 70,
          "output_type": "execute_result",
          "data": {
            "text/plain": "+------------+-------------+\n|item[string]|     sum(_ts)|\n+------------+-------------+\n|       Salad|1566430357101|\n|       Tacos|1539174145337|\n|    Sandwich|1704314980807|\n|       Pizza|1593686821692|\n|        Soup|1612926241703|\n+------------+-------------+"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "# Run aggregations\r\n",
        "\r\n",
        "df.groupBy(df.item.string).sum().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "rosouzMongo",
              "session_id": 25,
              "statement_id": 73,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2020-10-21T19:39:01.5699021Z",
              "execution_start_time": "2020-10-21T19:39:27.6210745Z",
              "execution_finish_time": "2020-10-21T19:39:29.6538025Z"
            },
            "text/plain": "StatementMeta(rosouzMongo, 25, 73, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "execution_count": 71,
          "output_type": "execute_result",
          "data": {
            "text/plain": "<bound method DataFrame.printSchema of DataFrame[_rid: string, _ts: bigint, id: string, _etag: string, _id: struct<objectId:string>, item: struct<string:string>, price: struct<float64:double>, rating: struct<int32:int>, timestamp: struct<float64:double,string:string>, _partitionKey: struct<string:string>]>"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "# Check the DataFrame schema\r\n",
        "\r\n",
        "df.printSchema"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Schema Representation - A quick note about the MongoDB schema in analytical store\r\n",
        "\r\n",
        "Please note in the result above that for the `timestamp` field we have only 1 datatype: `struct<float64:double>`. \r\n",
        "We will see that this detail will change since we will insert data with different datatype for that `timestamp` field.\r\n",
        "\r\n",
        "For MongoDB accounts we make use of a **Full Fidelity Schema** as a default option. This is a representation of property names extended with their data types to provide an accurate \r\n",
        "representation of their values and avoid ambiguity.\r\n",
        "\r\n",
        "This is why, when we called the fields above, we used their datatype as a suffix. Like in the example below:\r\n",
        "\r\n",
        "```\r\n",
        "df.filter((df.item.string == \"Pizza\")).show(10)\r\n",
        "```\r\n",
        "\r\n",
        "Notice how we specified the `string` type after the name of the property. Here is a map of all potential properties and their suffix representations in the Analytical Store:\r\n",
        "\r\n",
        "| Original Data Type     &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;| Suffix    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;| Example &nbsp;&nbsp;&nbsp;&nbsp; | \r\n",
        "|---------------|----------------|--------|\r\n",
        "| Double        | \".float64\"     |  `24.99`   |\r\n",
        "| Array         | \".array\"       |  `[\"a\", \"b\"]`   |\r\n",
        "| Binary        | \".binary\"      |  `0`   |\r\n",
        "| Boolean       | \".bool\"        |  `True`   |\r\n",
        "| Int32         | \".int32\"       |  `123`   |\r\n",
        "| Int64         | \".int64\"       |  `255486129307`   |\r\n",
        "| Null          | \".null\"        |  `null`   |\r\n",
        "| String        | \".string\"      |  `\"ABC\"`   |\r\n",
        "| Timestamp     | \".timestamp\"   |  `Timestamp(0, 0)`   |\r\n",
        "| DateTime      | \".date\"        |  `ISODate(\"2020-08-21T07:43:07.375Z\")`   |\r\n",
        "| ObjectId      | \".objectId\"    |  `ObjectId(\"5f3f7b59330ec25c132623a2\")`   |\r\n",
        "| Document      | \".object\"      |  `{\"a\": \"a\"}`   |\r\n",
        "\r\n",
        "These types are inferred from the data that is inserted in the transactional store. You can see the schema by executing the following command:\r\n",
        "```\r\n",
        "df.printSchema\r\n",
        "```\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "For more information about schemas representation, click [here](https://docs.microsoft.com/azure/cosmos-db/analytical-store-introduction#schema-representation) .\r\n",
        "\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Let's insert more orders!\n",
        "\n",
        "This time we will use slightly different data. Each item will contain the following fields:\n",
        "- item, string\n",
        "- price, float\n",
        "- rating, integer\n",
        "- timestamp, [ISO String format](https://en.wikipedia.org/wiki/ISO_8601)\n",
        "\n",
        "Notice how the `Timestamp` field is now in a string format. This will help us understand how the different data fields can be read based on their data type.\n",
        "\n",
        "After that, we will load the data, check the schema, and run some queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "rosouzMongo",
              "session_id": 25,
              "statement_id": 74,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2020-10-21T19:39:01.678759Z",
              "execution_start_time": "2020-10-21T19:39:29.6743381Z",
              "execution_finish_time": "2020-10-21T19:39:45.9294536Z"
            },
            "text/plain": "StatementMeta(rosouzMongo, 25, 74, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "execution_count": 72,
          "output_type": "execute_result",
          "data": {
            "text/plain": "finished creating 500 orders"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "from random import randint\n",
        "from time import strftime\n",
        "\n",
        "orders = db[\"HTAP\"]\n",
        "\n",
        "items = ['Pizza','Sandwich','Soup', 'Salad', 'Tacos']\n",
        "prices = [2.99, 3.49, 5.49, 12.99, 54.49]\n",
        "\n",
        "for x in range(1, 501):\n",
        "    order = {\n",
        "        'item' : items[randint(0, (len(items)-1))],\n",
        "        'price' : prices[randint(0, (len(prices)-1))],\n",
        "        'rating' : randint(1, 5),\n",
        "        'timestamp' : strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    }\n",
        "    \n",
        "    result=orders.insert(order)\n",
        "\n",
        "print('finished creating 500 orders')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Let's read the data and check the schema again!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "rosouzMongo",
              "session_id": 25,
              "statement_id": 75,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2020-10-21T19:39:01.7893871Z",
              "execution_start_time": "2020-10-21T19:39:45.9485444Z",
              "execution_finish_time": "2020-10-21T19:39:47.9821813Z"
            },
            "text/plain": "StatementMeta(rosouzMongo, 25, 75, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "execution_count": 73,
          "output_type": "execute_result",
          "data": {
            "text/plain": "<bound method DataFrame.printSchema of DataFrame[_rid: string, _ts: bigint, id: string, _etag: string, _id: struct<objectId:string>, item: struct<string:string>, price: struct<float64:double>, rating: struct<int32:int>, timestamp: struct<float64:double,string:string>, _partitionKey: struct<string:string>]>"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "# Load the Analytical Store data into a dataframe\n",
        "# Make sure to run the cell with the secrets to get the DATABASE_ACCOUNT_NAME and the DATABASE_ACCOUNT_READWRITE_KEY variables.\n",
        "df = spark.read.format(\"cosmos.olap\")\\\n",
        "    .option(\"spark.cosmos.accountEndpoint\", \"https://{account}.documents.azure.com:443/\".format(account = DATABASE_ACCOUNT_NAME))\\\n",
        "    .option(\"spark.cosmos.accountKey\", DATABASE_ACCOUNT_READWRITE_KEY)\\\n",
        "    .option(\"spark.cosmos.database\", DATABASE_NAME)\\\n",
        "    .option(\"spark.cosmos.container\", COLLECTION_NAME)\\\n",
        "    .load()\n",
        "\n",
        "# Check the schema AGAIN. Try to find something different.\n",
        "df.printSchema"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Schema Representation - What Changed?\r\n",
        "\r\n",
        "Please note in the result above that now, for the `timestamp` field, we have 2 datatypes: `struct<float64:double>` and `string:string`. That happened because we added data with a different datatype. That's `Full Fidelity Schema`, Azure Cosmos DB will do a full representation of your data, with the datatypes you used.\r\n",
        "\r\n",
        "## Queries\r\n",
        "\r\n",
        "Now let's run some interesting queries, using the datypes to filter the data.\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "rosouzMongo",
              "session_id": 25,
              "statement_id": 76,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2020-10-21T19:39:01.9029564Z",
              "execution_start_time": "2020-10-21T19:39:48.0046009Z",
              "execution_finish_time": "2020-10-21T19:39:50.0358426Z"
            },
            "text/plain": "StatementMeta(rosouzMongo, 25, 76, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "execution_count": 74,
          "output_type": "execute_result",
          "data": {
            "text/plain": "+-------------------------------+--------+\n|sum(price.float64 AS `float64`)|count(1)|\n+-------------------------------+--------+\n|              6125.269999999993|     423|\n+-------------------------------+--------+"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "# SQL!!\r\n",
        "# Let's see the data for pizzas that have a string timestamp\r\n",
        "df.createOrReplaceTempView(\"Pizza\")\r\n",
        "sql_results = spark.sql(\"SELECT sum(price.float64),count(*) FROM Pizza where timestamp.string is not null and item.string = 'Pizza'\")\r\n",
        "sql_results.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "rosouzMongo",
              "session_id": 25,
              "statement_id": 77,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2020-10-21T19:39:02.0382034Z",
              "execution_start_time": "2020-10-21T19:39:50.0558863Z",
              "execution_finish_time": "2020-10-21T19:39:52.0977798Z"
            },
            "text/plain": "StatementMeta(rosouzMongo, 25, 77, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "execution_count": 75,
          "output_type": "execute_result",
          "data": {
            "text/plain": "+-------------------------------+--------+\n|sum(price.float64 AS `float64`)|count(1)|\n+-------------------------------+--------+\n|              9014.289999999995|     571|\n+-------------------------------+--------+"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "# SQL!!\r\n",
        "# Let's see the data for pizzas that have a string timestamp\r\n",
        "df.createOrReplaceTempView(\"Pizza\")\r\n",
        "sql_results = spark.sql(\"SELECT sum(price.float64),count(*) FROM Pizza where timestamp.float64 is not null and item.string = 'Pizza'\")\r\n",
        "sql_results.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "rosouzMongo",
              "session_id": 25,
              "statement_id": 78,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2020-10-21T19:39:02.1182075Z",
              "execution_start_time": "2020-10-21T19:39:52.1196582Z",
              "execution_finish_time": "2020-10-21T19:39:54.1387573Z"
            },
            "text/plain": "StatementMeta(rosouzMongo, 25, 78, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "execution_count": 76,
          "output_type": "execute_result",
          "data": {
            "text/plain": "+-----------------------------------+---------------------------------+\n|max(timestamp.float64 AS `float64`)|max(timestamp.string AS `string`)|\n+-----------------------------------+---------------------------------+\n|                1.603308993772331E9|              2020-10-21 19:36:56|\n+-----------------------------------+---------------------------------+"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "# SQL!!\r\n",
        "# Let's compare both timestamp columns\r\n",
        "df.createOrReplaceTempView(\"Pizza\")\r\n",
        "sql_results = spark.sql(\"SELECT max(timestamp.float64),max(timestamp.string) FROM Pizza where item.string = 'Pizza'\")\r\n",
        "sql_results.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Schema Representation - Last thoughts\r\n",
        "\r\n",
        "Please note that the queries above return different data because of the filters on the timestamp column. From the user perspective, it's like there are 2 different columns, `timestamp.float64` and `timestamp.string`.\r\n",
        "\r\n",
        "## Conclusion\r\n",
        "\r\n",
        "Now you know how to use Azure Synapse Link for Azure Cosmos DB analitical store for MongoDB API. Also, now you know how to work with dataframes, full fidelity schema, and Spark Sql."
      ]
    }
  ]
}